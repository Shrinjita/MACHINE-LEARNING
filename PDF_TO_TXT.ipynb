{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9mMWB5LE73Je8yA0Tdzru",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shrinjita/MACHINE-LEARNING/blob/main/PDF_TO_TXT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIInVNQ95BF-",
        "outputId": "1a5014c5-f11d-4083-a410-f0a22d8fb543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFdBAGxF5JlP",
        "outputId": "9e3b3747-b0ab-4343-8dcc-7fb22dadec34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = '/content/drive/My Drive/Colab Notebooks/NNML CONVERT/Sustainable Development Goals.pdf'\n"
      ],
      "metadata": {
        "id": "lsS8zykY7nIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag-Xq1Xw4708",
        "outputId": "736d01b5-e2a7-40a8-b079-b528f7f7bc40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNITED NATIONS DEVELOPMENT PROGRAMME\n",
            "Sustainable Development GoalsSustainable Development Goals \n",
            "Facts & Figures\n",
            "United Nations Development ProgrammeWhat are the Sustainable \n",
            "Development Goals? \n",
            "https:// www.undp.org /content/ undp /en/home/s\n",
            "ustainable -development -goals.html•The Sustainable Development Goals (SDGs), also known asthe\n",
            "Global Goals, were adopted byallUnited Nations Member\n",
            "States in2015 asauniversal call toaction toend poverty,\n",
            "protect theplanet and ensure that allpeople enjoy peace and\n",
            "prosperity by2030 .\n",
            "•The 17SDGs areintegrated —that is,they recognize that action\n",
            "inone area will affect outcomes inothers, and that\n",
            "development must balance social, economic and\n",
            "environmental sustainability .\n",
            "•Through the pledge toLeave NoOne Behind, countries have\n",
            "committed tofast-track progress forthose furthest behind\n",
            "first.That iswhy theSDGs aredesigned tobring theworld to\n",
            "several life-changing ‘zeros’, including zero poverty, hunger,\n",
            "AIDS and discrimination against women and girls.\n",
            "•Everyone isneeded toreach these ambitious targets .The\n",
            "creativity, knowhow, technology and financial resources from\n",
            "allofsociety isnecessary toachieve theSDGs inevery context .17 Goals\n",
            "https://sustainabledevelopment.un.or\n",
            "g/post2015/transformingourworld•Goal 1. End poverty in all its forms everywhere \n",
            "•Goal 2. End hunger, achieve food security and \n",
            "improved nutrition and promote sustainable \n",
            "agriculture \n",
            "•Goal 3. Ensure healthy lives and promote well -\n",
            "being for all at all ages \n",
            "•Goal 4. Ensure inclusive and equitable quality \n",
            "education and promote lifelong learning \n",
            "opportunities for all \n",
            "•Goal 5. Achieve gender equality and empower \n",
            "all women and girls \n",
            "•Goal 6. Ensure availability and sustainable \n",
            "management of water and sanitation for all 17 Goals\n",
            "https://sustainabledevelopment.un.or\n",
            "g/post2015/transformingourworld•Goal 7. Ensure access to affordable, reliable, \n",
            "sustainable and modern energy for all \n",
            "•Goal 8. Promote sustained, inclusive and sustainable \n",
            "economic growth, full and productive employment \n",
            "and decent work for all \n",
            "•Goal 9. Build resilient infrastructure, promote \n",
            "inclusive and sustainable industrialization and foster \n",
            "innovation \n",
            "•Goal 10. Reduce inequality within and among \n",
            "countries \n",
            "•Goal 11. Make cities and human settlements \n",
            "inclusive, safe, resilient and sustainable \n",
            "•Goal 12. Ensure sustainable consumption and \n",
            "production patterns17 Goals\n",
            "https://sustainabledevelopment.un.org/po\n",
            "st2015/transformingourworld•Goal 13. Take urgent action to combat climate change \n",
            "and its impacts \n",
            "•Goal 14. Conserve and sustainably use the oceans, seas \n",
            "and marine resources for sustainable development \n",
            "•Goal 15. Protect, restore and promote sustainable use of \n",
            "terrestrial ecosystems, sustainably manage forests, \n",
            "combat desertification, and halt and reverse land \n",
            "degradation and halt biodiversity loss \n",
            "•Goal 16. Promote peaceful and inclusive societies for \n",
            "sustainable development, provide access to justice for all \n",
            "and build effective, accountable and inclusive institutions \n",
            "at all levels \n",
            "•Goal 17. Strengthen the means of implementation and \n",
            "revitalize the global partnership for sustainable \n",
            "development 1. No Poverty\n",
            "https://www.undp.org/content/und\n",
            "p/en/home/sustainable -\n",
            "development -goals/goal -1-no-\n",
            "poverty.html•736 million people still are in extreme poverty.\n",
            "•10 percent of the world’s population live in \n",
            "extreme poverty, down from 36 percent in \n",
            "1990.\n",
            "•Some 1.3 billion people live in multidimensional \n",
            "poverty.\n",
            "•Half of all people living in poverty are under 18.\n",
            "•One person in every 10 is extremely poor.\n",
            "•80 percent of people living on less than $1.90 \n",
            "are in South Asia and sub -Saharan Africa.2. Zero Hunger\n",
            "https://www.undp.org/content/und\n",
            "p/en/home/sustainable -\n",
            "development -goals/goal -2-zero -\n",
            "hunger.html•The number of undernourished people \n",
            "reached 821 million in 2017.\n",
            "•In 2017 Asia accounted for nearly two thirds, \n",
            "63 percent, of the world’s hungry.\n",
            "•Nearly 151 million children under five, 22 \n",
            "percent, were still stunted in 2017.\n",
            "•More than 1 in 8 adults is obese.\n",
            "•1 in 3 women of reproductive age is anemic.\n",
            "•26 percent of workers are employed in \n",
            "agriculture.3. Good Health \n",
            "and Well -Being\n",
            "https://www.undp.org/content/und\n",
            "p/en/home/sustainable -\n",
            "development -goals/goal -3-good -\n",
            "health -and-well-being.html•At least 400 million people have no basic healthcare, and 40 percent \n",
            "lack social protection.\n",
            "•More than 1.6 billion people live in fragile settings where protracted \n",
            "crises, combined with weak national capacity to deliver basic health \n",
            "services, present a significant challenge to global health.\n",
            "•By the end of 2017, 21.7 million people living with HIV were receiving \n",
            "antiretroviral therapy. Yet more than 15 million people are still \n",
            "waiting for treatment.\n",
            "•Every 2 seconds someone aged 30 to 70 years dies prematurely from \n",
            "noncommunicable diseases -cardiovascular disease, chronic \n",
            "respiratory disease, diabetes or cancer.\n",
            "•7 million people die every year from exposure to fine particles in \n",
            "polluted air.\n",
            "•More than one of every three women have experienced either \n",
            "physical or sexual violence at some point in their life resulting in both \n",
            "short -and long -term consequences for their physical, mental, and \n",
            "sexual and reproductive health.4. Quality Education\n",
            "https://www.undp.org/content/undp/en/\n",
            "home/sustainable -development -\n",
            "goals/goal -4-quality -education.html•Enrollment in primary education in developing \n",
            "countries has reached 91 percent.\n",
            "•Still, 57 million primary -aged children remain out of \n",
            "school, more than half of them in sub -Saharan \n",
            "Africa.\n",
            "•In developing countries, one in four girls is not in \n",
            "school.\n",
            "•About half of all out -of-school children of primary \n",
            "school age live in conflict -affected areas.\n",
            "•103 million youth worldwide lack basic literacy skills, \n",
            "and more than 60 percent of them are women.\n",
            "•6 out of 10 children and adolescents are not \n",
            "achieving a minimum level of proficiency in reading \n",
            "and math.5. Gender Equality\n",
            "https://www.undp.org/content/undp/en\n",
            "/home/sustainable -development -\n",
            "goals/goal -5-gender -equality.html•Women earn only 77 cents for every dollar that \n",
            "men get for the same work.\n",
            "•35 percent of women have experienced physical \n",
            "and/or sexual violence.\n",
            "•Women represent just 13 percent of agricultural \n",
            "landholders.\n",
            "•Almost 750 million women and girls alive today \n",
            "were married before their 18th birthday.\n",
            "•Two thirds of developing countries have \n",
            "achieved gender parity in primary education.\n",
            "•Only 24 percent of national parliamentarians \n",
            "were women as of November 2018, a small \n",
            "increase from 11.3 percent in 1995.6. Clean Water and \n",
            "Sanitation \n",
            "https://www.undp.org/content/undp/en/ho\n",
            "me/sustainable -development -goals/goal -6-\n",
            "clean -water -and-sanitation.html•71 percent of the global population, 5.2 billion people, \n",
            "had safely -managed drinking water in 2015, but 844 \n",
            "million people still lacked even basic drinking water.\n",
            "•39 percent of the global population, 2.9 billion people, \n",
            "had safe sanitation in 2015, but 2.3 billion people still \n",
            "lacked basic sanitation. 892 million people practiced \n",
            "open defecation.\n",
            "•80 percent of wastewater goes into waterways without \n",
            "adequate treatment.\n",
            "•Water stress affects more than 2 billion people, with this \n",
            "figure projected to increase.\n",
            "•80 percent of countries have laid the foundations for \n",
            "integrated water resources management.\n",
            "•The world has lost 70 percent of its natural wetlands \n",
            "over the last century.7.Affordable and \n",
            "Clean Energy\n",
            "https://www.undp.org/content/undp/en/\n",
            "home/sustainable -development -\n",
            "goals/goal -7-affordable -and-clean -\n",
            "energy.html•One in 7 people still lacks electricity, and most of \n",
            "them live in rural areas of the developing world.\n",
            "•Energy is the main contributor to climate change, it \n",
            "produces around 60 percent of greenhouse gases.\n",
            "•More efficient energy standards could reduce \n",
            "building and industry electricity consumption by 14 \n",
            "percent.\n",
            "•More than 40 percent of the world’s population —3 \n",
            "billion —rely on polluting and unhealthy fuels for \n",
            "cooking.\n",
            "•As of 2015, more than 20 percent of power was \n",
            "generated through renewable sources.\n",
            "•The renewable energy sector employed a record \n",
            "10.3 million people in 2017.8. Decent Work and \n",
            "Economic Growth\n",
            "https://www.undp.org/content/undp/en/h\n",
            "ome/sustainable -development -goals/goal -\n",
            "8-decent -work -and-economic -growth.html•An estimated 172 million people worldwide were without \n",
            "work in 2018 -an unemployment rate of 5 percent.\n",
            "•As a result of an expanding labor force, the number of \n",
            "unemployed is projected to increase by 1 million every year \n",
            "and reach 174 million by 2020.\n",
            "•Some 700 million workers lived in extreme or moderate \n",
            "poverty in 2018, with less than US$3.20 per day.\n",
            "•Women’s participation in the labor force stood at 48 per cent \n",
            "in 2018, compared with 75 percent for men. Around 3 in 5 of \n",
            "the 3.5 billion people in the labor force in 2018 were men.\n",
            "•Overall, 2 billion workers were in informal employment in \n",
            "2016, accounting for 61 per cent of the world’s workforce.\n",
            "•Many more women than men are underutilized in the labor \n",
            "force —85 million compared to 55 million.9.Industry, Innovation \n",
            "and Infrastructure\n",
            "https://www.undp.org/content/undp/en/home/\n",
            "sustainable -development -goals/goal -9-industry -\n",
            "innovation -and-infrastructure.htmlIndustry, \n",
            "Innovation, and Infrastructure•Worldwide, 2.3 billion people lack access to basic \n",
            "sanitation.\n",
            "•In some low -income African countries, infrastructure \n",
            "constraints cut businesses’ productivity by around 40 \n",
            "percent.\n",
            "•2.6 billion people in developing countries do not have \n",
            "access to constant electricity.\n",
            "•More than 4 billion people still do not have access to the \n",
            "Internet; 90 percent of them are in the developing \n",
            "world.\n",
            "•The renewable energy sectors currently employ more \n",
            "than 2.3 million people; the number could reach 20 \n",
            "million by 2030.\n",
            "•In developing countries, barely 30 percent of agricultural \n",
            "products undergo industrial processing, compared to 98 \n",
            "percent high -income countries.10. Reduced Inequality\n",
            "https://www.undp.org/content/undp/en/home/\n",
            "sustainable -development -goals/goal -10-\n",
            "reduced -inequalities.html•In 2016, 22 percent of global income was received by the top 1 \n",
            "percent compared with 10 percent of income for the bottom \n",
            "50 percent.\n",
            "•In 1980, the top one percent had 16 percent of global income. \n",
            "The bottom 50 percent had 8 percent of income.\n",
            "•Economic inequality is largely driven by the unequal ownership \n",
            "of capital. Since 1980, very large transfers of public to private \n",
            "wealth occurred in nearly all countries. The global wealth \n",
            "share of the top 1 percent was 33 percent in 2016.\n",
            "•Under \"business as usual\", the top 1 percent global wealth will \n",
            "reach 39 percent by 2050.\n",
            "•Women spend, on average, twice as much time on unpaid \n",
            "housework as men.\n",
            "•Women have as much access to financial services as men in \n",
            "just 60 percent of the countries assessed and to land \n",
            "ownership in just 42 percent of the countries assessed.11. Sustainable Cities \n",
            "and Communities\n",
            "https://www.undp.org/content/undp/en/ho\n",
            "me/sustainable -development -goals/goal -11-\n",
            "sustainable -cities -and-communities.html•In 2018, 4.2 billion people, 55 percent of the world’s \n",
            "population, lived in cities. By 2050, the urban population \n",
            "is expected to reach 6.5 billion.\n",
            "•Cities occupy just 3 percent of the Earth’s land but \n",
            "account for 60 to 80 percent of energy consumption and \n",
            "at least 70 percent of carbon emissions.\n",
            "•828 million people are estimated to live in slums, and the \n",
            "number is rising.\n",
            "•In 1990, there were 10 cities with 10 million people or \n",
            "more; by 2014, the number of mega -cities rose to 28, \n",
            "and was expected to reach 33 by 2018. In the future, 9 \n",
            "out of 10 mega -cities will be in the developing world.\n",
            "•In the coming decades, 90 percent of urban expansion \n",
            "will be in the developing world.\n",
            "•The economic role of cities is significant. They generate \n",
            "about 80 percent of the global GDP .12. Responsible \n",
            "Consumption and \n",
            "Production\n",
            "https://www.undp.org/content/undp/en/home/\n",
            "sustainable -development -goals/goal -12-\n",
            "responsible -consumption -and-production.html•1.3 billion tonnes of food is wasted every year, while \n",
            "almost 2 billion people go hungry or undernourished.\n",
            "•The food sector accounts for around 22 percent of total \n",
            "greenhouse gas emissions, largely from the conversion of \n",
            "forests into farmland.\n",
            "•Globally, 2 billion people are overweight or obese.\n",
            "•Only 3 percent of the world’s water is fresh (drinkable), \n",
            "and humans are using it faster than nature can replenish \n",
            "it.\n",
            "•If people everywhere switched to energy efficient \n",
            "lightbulbs, the world would save US$120 billion annually.\n",
            "•One-fifth of the world’s final energy consumption in 2013 \n",
            "was from renewable sources.13. Climate Action\n",
            "https://www.undp.org/content/undp/en/h\n",
            "ome/sustainable -development -goals/goal -\n",
            "13-climate -action.html•As of 2017 humans are estimated to have caused \n",
            "approximately 1.0 °C of global warming above pre -industrial \n",
            "levels.\n",
            "•Sea levels have risen by about 20 cm (8 inches) since 1880 and \n",
            "are projected to rise another 30 –122 cm (1 to 4 feet) by 2100.\n",
            "•To limit warming to 1.5C, global net CO2 emissions must drop \n",
            "by 45% between 2010 and 2030, and reach net zero around \n",
            "2050.\n",
            "•Climate pledges under The Paris Agreement cover only one \n",
            "third of the emissions reductions needed to keep the world \n",
            "below 2 °C.\n",
            "•Bold climate action could trigger at least US$26 trillion in \n",
            "economic benefits by 2030.\n",
            "•The energy sector alone will create around 18 million more \n",
            "jobs by 2030, focused specifically on sustainable energy.14. Life Below Water\n",
            "https://www.undp.org/content/undp/en/ho\n",
            "me/sustainable -development -goals/goal -14-\n",
            "life-below -water.html•The ocean covers three quarters of the Earth’s surface \n",
            "and represents 99 percent of the living space on the \n",
            "planet by volume.\n",
            "•The ocean contains nearly 200,000 identified species, but \n",
            "actual numbers may lie in the millions.\n",
            "•As much as 40 percent of the ocean is heavily affected by \n",
            "pollution, depleted fisheries, loss of coastal habitats and \n",
            "other human activities.\n",
            "•The ocean absorbs about 30 percent of carbon dioxide \n",
            "produced by humans, buffering the impacts of global \n",
            "warming.\n",
            "•More than 3 billion people depend on marine and \n",
            "coastal biodiversity for their livelihoods.\n",
            "•The market value of marine and coastal resources and \n",
            "industries is estimated at US$3 trillion per year, about 5 \n",
            "percent of global GDP .15. Life on Land\n",
            "https://www.undp.org/content/und\n",
            "p/en/home/sustainable -\n",
            "development -goals/goal -15-life-on-\n",
            "land.html•Around 1.6 billion people depend on forests for \n",
            "their livelihoods.\n",
            "•Forests are home to more than 80 percent of all \n",
            "terrestrial species of animals, plants and \n",
            "insects.\n",
            "•2.6 billion people depend directly on agriculture \n",
            "for a living.\n",
            "•Nature -based climate solutions can contribute \n",
            "about a third of CO2 reductions by 2030.\n",
            "•The value of ecosystems to human livelihoods \n",
            "and well -being is $US125 trillion per year.\n",
            "•Mountain regions provide 60 -80 percent of the \n",
            "Earth's fresh water.16. Peace, Justice, and \n",
            "Strong Institutions\n",
            "https:// www.undp.org /content/ undp /en/home/sustain\n",
            "able -development -goals/goal -16-peace -justice -and-\n",
            "strong -institutions.html•By the end of 2017, 68.5 million people had been forcibly \n",
            "displaced as a result of persecution, conflict, violence or \n",
            "human rights violations.\n",
            "•There are at least 10 million stateless people who have \n",
            "been denied nationality and its related rights.\n",
            "•Corruption, bribery, theft and tax evasion cost \n",
            "developing countries US$1.26 trillion per year.\n",
            "•49 countries lack laws protecting women from domestic \n",
            "violence.\n",
            "•In 46 countries, women now hold more than 30 percent \n",
            "of seats in at least one chamber of national parliament.\n",
            "•1 billion people are legally ‘invisible’ because they cannot \n",
            "prove who they are. This includes an estimated 625 \n",
            "million children under 14 whose births were never \n",
            "registered.17. Partnerships for \n",
            "the Goals\n",
            "https://www.undp.org/content/undp/en/\n",
            "home/sustainable -development -\n",
            "goals/goal -17-partnerships -for-the-\n",
            "goals.html•The UN Conference on Trade and Development \n",
            "(UNCTAD) says achieving SDGs will require US$5 trillion \n",
            "to $7 trillion in annual investment.\n",
            "•Total official development assistance reached US$147.2 \n",
            "billion in 2017.\n",
            "•In 2017, international remittances totaled US$613 billion; \n",
            "76 percent of it went to developing countries.\n",
            "•In 2016, 6 countries met the international target to keep \n",
            "official development assistance at or above 0.7 percent \n",
            "of gross national income.\n",
            "•Sustainable and responsible investments represent high -\n",
            "potential sources of capital for SDGs. As of 2016, \n",
            "US$18.2 trillion was invested in this asset class.\n",
            "•The bond market for sustainable business is growing. In \n",
            "2018 global green bonds reached US$155.5billion, up 78 \n",
            "percent from previous year.THANK YOU\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    text = ''\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        num_pages = len(reader.pages)\n",
        "        for page_num in range(num_pages):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "extracted_text = pdf_to_text(pdf_path)\n",
        "print(extracted_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LNNx6E0MuJJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "9jpmfB59d-Zf",
        "outputId": "30b931b9-ea4f-433a-9170-5c36ff842e5b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-776622d2-1800-41d8-a0e9-427c4cfb77c5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-776622d2-1800-41d8-a0e9-427c4cfb77c5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-21dc3c638f66>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    157\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    158\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(open(filename, 'rb'))\n",
        "\n",
        "# Now you can work with the DataFrame 'df'\n"
      ],
      "metadata": {
        "id": "yoIb6YGTd4NI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuRwrKPaeo9s",
        "outputId": "39dc06b2-951f-42b8-e003-40efda186126"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Project Code', 'Project Title', 'Project Description', 'Faculty',\n",
            "       'Application URL', 'Project Mode'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "EltlKswhf2U-",
        "outputId": "14d3c77b-71be-4737-8641-181dffa339f0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Project Code                                      Project Title  \\\n",
              "0  IP0ALC0000010              Laser-based ambient methane detection   \n",
              "1  IP0ALC0000011  Development of a optical fiber tip-based plasm...   \n",
              "2            NaN                                                NaN   \n",
              "3            NaN                                                NaN   \n",
              "4  IP0ALC0000012  Deep learning-based automatic speech recogniti...   \n",
              "5            NaN                                                NaN   \n",
              "6            NaN                                                NaN   \n",
              "7   IP0AM0000004  De novo creation of organic emitters via graph...   \n",
              "8   IP0AM0000005  Designing Efficient Organic Solar Cells with M...   \n",
              "9   IP0AM0000006  Machine-Learning Driven Optimization of Lennar...   \n",
              "\n",
              "                                 Project Description               Faculty  \\\n",
              "0  The Photonic Sensors Lab specializes in the de...  Arup Lal Chakraborty   \n",
              "1  We are currently developing an optical fiber-b...  Arup Lal Chakraborty   \n",
              "2                                                NaN                   NaN   \n",
              "3  Background requirement: Some experience with o...                   NaN   \n",
              "4  Speech recognition, also called automatic spee...  Arup Lal Chakraborty   \n",
              "5                                                NaN                   NaN   \n",
              "6  Prerequisites: Well-versed in Python. Should h...                   NaN   \n",
              "7  The development of machine learning-based tech...        Anirban Mondal   \n",
              "8  Due to the complex nature of organic material ...        Anirban Mondal   \n",
              "9  Optimizing classical force field parameters, e...        Anirban Mondal   \n",
              "\n",
              "  Application URL    Project Mode  \n",
              "0      Apply Here  In Person Mode  \n",
              "1      Apply Here  In Person Mode  \n",
              "2             NaN             NaN  \n",
              "3             NaN             NaN  \n",
              "4      Apply Here  In Person Mode  \n",
              "5             NaN             NaN  \n",
              "6             NaN             NaN  \n",
              "7      Apply Here  In Person Mode  \n",
              "8      Apply Here  In Person Mode  \n",
              "9      Apply Here  In Person Mode  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-848744ad-b02b-44d6-a875-9177687fa746\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Project Code</th>\n",
              "      <th>Project Title</th>\n",
              "      <th>Project Description</th>\n",
              "      <th>Faculty</th>\n",
              "      <th>Application URL</th>\n",
              "      <th>Project Mode</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IP0ALC0000010</td>\n",
              "      <td>Laser-based ambient methane detection</td>\n",
              "      <td>The Photonic Sensors Lab specializes in the de...</td>\n",
              "      <td>Arup Lal Chakraborty</td>\n",
              "      <td>Apply Here</td>\n",
              "      <td>In Person Mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IP0ALC0000011</td>\n",
              "      <td>Development of a optical fiber tip-based plasm...</td>\n",
              "      <td>We are currently developing an optical fiber-b...</td>\n",
              "      <td>Arup Lal Chakraborty</td>\n",
              "      <td>Apply Here</td>\n",
              "      <td>In Person Mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Background requirement: Some experience with o...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IP0ALC0000012</td>\n",
              "      <td>Deep learning-based automatic speech recogniti...</td>\n",
              "      <td>Speech recognition, also called automatic spee...</td>\n",
              "      <td>Arup Lal Chakraborty</td>\n",
              "      <td>Apply Here</td>\n",
              "      <td>In Person Mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Prerequisites: Well-versed in Python. Should h...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>IP0AM0000004</td>\n",
              "      <td>De novo creation of organic emitters via graph...</td>\n",
              "      <td>The development of machine learning-based tech...</td>\n",
              "      <td>Anirban Mondal</td>\n",
              "      <td>Apply Here</td>\n",
              "      <td>In Person Mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>IP0AM0000005</td>\n",
              "      <td>Designing Efficient Organic Solar Cells with M...</td>\n",
              "      <td>Due to the complex nature of organic material ...</td>\n",
              "      <td>Anirban Mondal</td>\n",
              "      <td>Apply Here</td>\n",
              "      <td>In Person Mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>IP0AM0000006</td>\n",
              "      <td>Machine-Learning Driven Optimization of Lennar...</td>\n",
              "      <td>Optimizing classical force field parameters, e...</td>\n",
              "      <td>Anirban Mondal</td>\n",
              "      <td>Apply Here</td>\n",
              "      <td>In Person Mode</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-848744ad-b02b-44d6-a875-9177687fa746')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-848744ad-b02b-44d6-a875-9177687fa746 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-848744ad-b02b-44d6-a875-9177687fa746');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bbc6ba9e-171b-46ab-95d4-edcfea1503b3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bbc6ba9e-171b-46ab-95d4-edcfea1503b3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bbc6ba9e-171b-46ab-95d4-edcfea1503b3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 255,\n  \"fields\": [\n    {\n      \"column\": \"Project Code\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 131,\n        \"samples\": [\n          \"IP0LL0000014\",\n          \"IP0KKD0000006\",\n          \"IP0CKM0000007\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Project Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 131,\n        \"samples\": [\n          \"Developing Artificial Touch Paradigms\",\n          \"Influence of non-thermal active fluctuations over colloidal dynamics under crowded conditions\",\n          \"Establishing protocols for performing optical microscopy experiments with E.coli\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Project Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 215,\n        \"samples\": [\n          \"The goal of this 8-week long project will be to: a) write python codes to post-process the existing atomistic simulation data for 2D materials based heterostructure from QuantumATK, and b) Build deep-learning models for the inverse design of the Metal-2D material heterostructure to achieve an optimum contact resistance. The project will require a strong grasp on python and an extensive experience in building deep-learning models. Looking forward to having an individual with a strong interest in applying machine learning to inverse design of atomistic 2D materials based devices.\",\n          \"Paper Reference: https://doi.org/10.1109/ICRA40945.2020.9197279 and\",\n          \"Remaining useful lifetime (RUL) of the battery is an important parameter that is needed for battery reliability and finds applications in battery swapping and second lifetime applications along with performance optimization. Machine learning provides an accurate tool of data-driven battery lifetime estimation using internal state estimation with limited real-field measurements. In this project students will work on pre-collected datasets and develop RUL estimation algorithms using ML.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Faculty\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          \"Shanmuganathan Raman\",\n          \"Uddipta Ghosh\",\n          \"Arup Lal Chakraborty\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Application URL\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Apply Here\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Project Mode\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"In Person Mode\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = df.drop('Application URL', axis=1)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "jk04v-ZAlIKO",
        "outputId": "bb4b77b5-5e14-4555-e65b-34f0286db2f5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Project Code                                      Project Title  \\\n",
              "0    IP0ALC0000010              Laser-based ambient methane detection   \n",
              "1    IP0ALC0000011  Development of a optical fiber tip-based plasm...   \n",
              "2              NaN                                                NaN   \n",
              "3              NaN                                                NaN   \n",
              "4    IP0ALC0000012  Deep learning-based automatic speech recogniti...   \n",
              "..             ...                                                ...   \n",
              "250   IP0VV0000013                        Cable driven parallel robot   \n",
              "251  IP0YKM0000001  Virtual reality-based interfaces for effective...   \n",
              "252  IP0YKM0000002  Hands-Free Text-based user interface for iOS a...   \n",
              "253  IP0YKM0000003  Speech and Natural Language Input for Digital ...   \n",
              "254  IP0YKM0000004                                  Conversational AI   \n",
              "\n",
              "                                   Project Description               Faculty  \\\n",
              "0    The Photonic Sensors Lab specializes in the de...  Arup Lal Chakraborty   \n",
              "1    We are currently developing an optical fiber-b...  Arup Lal Chakraborty   \n",
              "2                                                  NaN                   NaN   \n",
              "3    Background requirement: Some experience with o...                   NaN   \n",
              "4    Speech recognition, also called automatic spee...  Arup Lal Chakraborty   \n",
              "..                                                 ...                   ...   \n",
              "250  We are developing a table-top cable-driven par...       Vineet Vashista   \n",
              "251                                                NaN        Yogesh K Meena   \n",
              "252                                                NaN        Yogesh K Meena   \n",
              "253                                                NaN        Yogesh K Meena   \n",
              "254                                                NaN        Yogesh K Meena   \n",
              "\n",
              "       Project Mode  \n",
              "0    In Person Mode  \n",
              "1    In Person Mode  \n",
              "2               NaN  \n",
              "3               NaN  \n",
              "4    In Person Mode  \n",
              "..              ...  \n",
              "250  In Person Mode  \n",
              "251  In Person Mode  \n",
              "252  In Person Mode  \n",
              "253  In Person Mode  \n",
              "254             NaN  \n",
              "\n",
              "[255 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a14cf89a-c2bc-4848-bcbf-f4f3a0b5f44b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Project Code</th>\n",
              "      <th>Project Title</th>\n",
              "      <th>Project Description</th>\n",
              "      <th>Faculty</th>\n",
              "      <th>Project Mode</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IP0ALC0000010</td>\n",
              "      <td>Laser-based ambient methane detection</td>\n",
              "      <td>The Photonic Sensors Lab specializes in the de...</td>\n",
              "      <td>Arup Lal Chakraborty</td>\n",
              "      <td>In Person Mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IP0ALC0000011</td>\n",
              "      <td>Development of a optical fiber tip-based plasm...</td>\n",
              "      <td>We are currently developing an optical fiber-b...</td>\n",
              "      <td>Arup Lal Chakraborty</td>\n",
              "      <td>In Person Mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Background requirement: Some experience with o...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IP0ALC0000012</td>\n",
              "      <td>Deep learning-based automatic speech recogniti...</td>\n",
              "      <td>Speech recognition, also called automatic spee...</td>\n",
              "      <td>Arup Lal Chakraborty</td>\n",
              "      <td>In Person Mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>IP0VV0000013</td>\n",
              "      <td>Cable driven parallel robot</td>\n",
              "      <td>We are developing a table-top cable-driven par...</td>\n",
              "      <td>Vineet Vashista</td>\n",
              "      <td>In Person Mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>IP0YKM0000001</td>\n",
              "      <td>Virtual reality-based interfaces for effective...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yogesh K Meena</td>\n",
              "      <td>In Person Mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>IP0YKM0000002</td>\n",
              "      <td>Hands-Free Text-based user interface for iOS a...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yogesh K Meena</td>\n",
              "      <td>In Person Mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>IP0YKM0000003</td>\n",
              "      <td>Speech and Natural Language Input for Digital ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yogesh K Meena</td>\n",
              "      <td>In Person Mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>IP0YKM0000004</td>\n",
              "      <td>Conversational AI</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yogesh K Meena</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>255 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a14cf89a-c2bc-4848-bcbf-f4f3a0b5f44b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a14cf89a-c2bc-4848-bcbf-f4f3a0b5f44b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a14cf89a-c2bc-4848-bcbf-f4f3a0b5f44b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-55196ba0-99c8-49c5-8dda-baabcfa0b599\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55196ba0-99c8-49c5-8dda-baabcfa0b599')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-55196ba0-99c8-49c5-8dda-baabcfa0b599 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 255,\n  \"fields\": [\n    {\n      \"column\": \"Project Code\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 131,\n        \"samples\": [\n          \"IP0LL0000014\",\n          \"IP0KKD0000006\",\n          \"IP0CKM0000007\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Project Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 131,\n        \"samples\": [\n          \"Developing Artificial Touch Paradigms\",\n          \"Influence of non-thermal active fluctuations over colloidal dynamics under crowded conditions\",\n          \"Establishing protocols for performing optical microscopy experiments with E.coli\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Project Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 215,\n        \"samples\": [\n          \"The goal of this 8-week long project will be to: a) write python codes to post-process the existing atomistic simulation data for 2D materials based heterostructure from QuantumATK, and b) Build deep-learning models for the inverse design of the Metal-2D material heterostructure to achieve an optimum contact resistance. The project will require a strong grasp on python and an extensive experience in building deep-learning models. Looking forward to having an individual with a strong interest in applying machine learning to inverse design of atomistic 2D materials based devices.\",\n          \"Paper Reference: https://doi.org/10.1109/ICRA40945.2020.9197279 and\",\n          \"Remaining useful lifetime (RUL) of the battery is an important parameter that is needed for battery reliability and finds applications in battery swapping and second lifetime applications along with performance optimization. Machine learning provides an accurate tool of data-driven battery lifetime estimation using internal state estimation with limited real-field measurements. In this project students will work on pre-collected datasets and develop RUL estimation algorithms using ML.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Faculty\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          \"Shanmuganathan Raman\",\n          \"Uddipta Ghosh\",\n          \"Arup Lal Chakraborty\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Project Mode\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"In Person Mode\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2=df['Project Title']"
      ],
      "metadata": {
        "id": "-YDIL06-mn54"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=df2.dropna()"
      ],
      "metadata": {
        "id": "_jQkdkBTf5EB"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df2.iloc[0:10])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHTtVkRUnPqR",
        "outputId": "6c5f5d48-4ee6-4693-8908-5139baa6283b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                 Laser-based ambient methane detection\n",
            "1     Development of a optical fiber tip-based plasm...\n",
            "4     Deep learning-based automatic speech recogniti...\n",
            "7     De novo creation of organic emitters via graph...\n",
            "8     Designing Efficient Organic Solar Cells with M...\n",
            "9     Machine-Learning Driven Optimization of Lennar...\n",
            "10    Curation and analysis of multi-omic data on ci...\n",
            "11    Virtual screening of small molecules against t...\n",
            "12    Alloy catalyst synthesis and characterization ...\n",
            "13                          LHC Analysis of BSM Physics\n",
            "Name: Project Title, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=len(df2)\n",
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW_gv-4EokVA",
        "outputId": "debd1782-b6b0-4125-95e1-e6a980b17644"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRnUYOWZooae",
        "outputId": "b99d3780-258a-40ff-e2ea-90ce6c216855"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                  Laser-based ambient methane detection\n",
              "1      Development of a optical fiber tip-based plasm...\n",
              "4      Deep learning-based automatic speech recogniti...\n",
              "7      De novo creation of organic emitters via graph...\n",
              "8      Designing Efficient Organic Solar Cells with M...\n",
              "                             ...                        \n",
              "250                          Cable driven parallel robot\n",
              "251    Virtual reality-based interfaces for effective...\n",
              "252    Hands-Free Text-based user interface for iOS a...\n",
              "253    Speech and Natural Language Input for Digital ...\n",
              "254                                    Conversational AI\n",
              "Name: Project Title, Length: 131, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Icq0qkYVuigY",
        "outputId": "cc337f8e-6faa-46e7-fbf5-ce30f488dc13"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/Colab Notebooks/NNML/output.csv'\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "df2.to_csv(file_path, index=False)  # Set index=False if you don't want to write row indices to the file\n"
      ],
      "metadata": {
        "id": "saUkAnMqozrh"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZakEiyuFiiX8",
        "outputId": "6ccbc332-2de8-4011-f2a7-d82bcee2956e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Function to generate more information about a topic using GPT-2\n",
        "def generate_information(topic):\n",
        "    model_name = \"gpt2\"  # You can choose a different GPT-2 variant if needed\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    prompt = f\"Provide more information about {topic}.\"\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "    # Generate text using GPT-2\n",
        "    outputs = model.generate(inputs, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "    generated_info = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    return generated_info\n",
        "\n",
        "# Read CSV file with topics\n",
        "csv_file_path = 'output_file.csv'  # Update with your CSV file path\n",
        "with open(csv_file_path, 'r') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "\n",
        "    # Assuming the column containing topics is named 'topic'\n",
        "    for row in reader:\n",
        "        topic = row['topic']\n",
        "        extracted_data = df2\n",
        "\n",
        "        # Generate more information about the topic using GPT-2 (or another model)\n",
        "        generated_info = generate_information(topic)\n",
        "\n",
        "        # Output the results (you can modify this based on your needs)\n",
        "        print(f\"Topic: {topic}\\nExtracted Data: {extracted_data}\\nGenerated Information: {generated_info}\\n{'=' * 50}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "iadum7FqilXU",
        "outputId": "3ec59213-b48c-493b-f324-c491bcd55550"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'output_file.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-a44a1650ffd9>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Read CSV file with topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcsv_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'output_file.csv'\u001b[0m  \u001b[0;31m# Update with your CSV file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output_file.csv'"
          ]
        }
      ]
    }
  ]
}